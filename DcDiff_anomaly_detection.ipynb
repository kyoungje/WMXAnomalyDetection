{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Autoencoder Anomaly Detection\n",
    "## Communication error detection through **WMX time-series data**\n",
    "\n",
    "**DC diff data gathered by the WMX ethercat master is used to detect anomalies in a time-series pattern.**\n",
    "\n",
    "In this notebook, we'll build an *LSTM Autoencoder representation learning model*. (See the following example of the autoencoder model.)\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/posts/2018-08-12-vae/autoencoder-architecture.png\" width=\"500\">\n",
    "\n",
    "You can find **excellent reference projects** that help you understand relevant concepts and techniques at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "### Install neccesary Python libraries\n",
    "Note that the Python environment where this notebook runs should already have **PyTorch** packages.\n",
    "\n",
    "(To install **PyTorch**, go to https://pytorch.org/get-started/locally/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install -U scikit-learn\n",
    "!pip install -q -U watermark\n",
    "!pip install datasets\n",
    "!pip install huggingface-hub\n",
    "!pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versions of the installed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -v -p numpy,pandas,torch,scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and initialize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the **normal** DC diff dataset from Hugging Face and load the dataset\n",
    "Log in to the Hugging Face before downloading\n",
    "(You may need a Hugging Face account to log in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to the Hugging face\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the normal dataset\n",
    "dataset = load_dataset(\"Jake5/wmx_normal_data\") \n",
    "\n",
    "hg_df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "print(f\"Total Values: {hg_df.shape[0]}\")\n",
    "print(hg_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if CUDA is available and use the CUDA available device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CUDA available={torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the current the first 1000 values of downloaded dataframe\n",
    "hg_df['Timestamp'] = pd.to_datetime(hg_df['Timestamp'])\n",
    "\n",
    "plt.plot(hg_df['Timestamp'].head(1000), hg_df['DcDiffAvg'].head(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess the dataset (Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy as dc\n",
    "\n",
    "def prepare_dataframe_for_lstm(df, n_steps):\n",
    "    df = dc(df)\n",
    "\n",
    "    df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "    for i in range(1, n_steps+1):\n",
    "        df[f'DcDiffAvg(t-{i})'] = df['DcDiffAvg'].shift(i)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "lookback = 10\n",
    "shifted_df = prepare_dataframe_for_lstm(hg_df, lookback)\n",
    "shifted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_df_as_np = shifted_df.to_numpy()\n",
    "\n",
    "shifted_df_as_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_df_as_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# min~max = -1~1\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "shifted_df_as_np = scaler.fit_transform(shifted_df_as_np)\n",
    "\n",
    "shifted_df_as_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = shifted_df_as_np[:, 1:]\n",
    "y = shifted_df_as_np[:, 0]\n",
    "\n",
    "# X is input values and y is target value\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse data to make the recent one last\n",
    "X = dc(np.flip(X, axis=1))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for train (95%) and test (5%)\n",
    "split_index = int(len(X) * 0.95)\n",
    "\n",
    "split_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:split_index]\n",
    "X_test = X[split_index:]\n",
    "\n",
    "y_train = y[:split_index]\n",
    "y_test = y[split_index:]\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((-1, lookback, 1))\n",
    "X_test = X_test.reshape((-1, lookback, 1))\n",
    "\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tensors using the train and test data\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if the shape of batch is appropriate\n",
    "for _, batch in enumerate(train_loader):\n",
    "    x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "    print(x_batch.shape, y_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a class for LSTM neural network\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_stacked_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Create the LSTM variable using input size = 1, hidden size = 4, number of stacked layers = 1 arguments\n",
    "#model = LSTM(1, 4, 1)\n",
    "model = LSTM(1, 128, 1)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(history):\n",
    "    # Train mode\n",
    "    model.train(True)\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "    running_loss = 0.0\n",
    "    train_losses = []\n",
    "\n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        output = model(x_batch)\n",
    "        loss = loss_function(output, y_batch)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 100 == 99:  # print every 100 batches\n",
    "            avg_loss_across_batches = running_loss / 100\n",
    "            train_losses.append(avg_loss_across_batches)\n",
    "            print('Batch {0}, Loss: {1:.3f}'.format(batch_index+1,\n",
    "                                                    avg_loss_across_batches))\n",
    "            running_loss = 0.0\n",
    "    print()\n",
    "    history['train'].append(np.mean(train_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(history):\n",
    "    # Not train mode\n",
    "    model.train(False)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_index, batch in enumerate(test_loader):\n",
    "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(x_batch)\n",
    "            loss = loss_function(output, y_batch)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    avg_loss_across_batches = running_loss / len(test_loader)\n",
    "    history['val'].append(avg_loss_across_batches)\n",
    "\n",
    "    print('Val Loss: {0:.3f}'.format(avg_loss_across_batches))\n",
    "    print('***************************************************')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training moodel (unless loading a pre-trained model later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "num_epochs = 60\n",
    "loss_function = nn.MSELoss()\n",
    "#loss_function = nn.L1Loss(reduction='sum').to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "history = dict(train=[], val=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validate\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(history)\n",
    "    validate_one_epoch(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot actual and predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predicted = model(X_train.to(device)).to('cpu').numpy()\n",
    "\n",
    "plt.plot(y_train[:2000], label='Actual DcDiffAvg')\n",
    "plt.plot(predicted[:2000], label='Predicted DcDiffAvg')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('DcDiffAvg')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss over training epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss function\n",
    "ax = plt.figure().gca()\n",
    "\n",
    "ax.plot(history['train'])\n",
    "ax.plot(history['val'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.title('Loss over training epochs')\n",
    "plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Assessment\n",
    "\n",
    "The training loss (blue line) shows a steady and stable learning curve, starting at around 0.021 and gradually decreasing to approximately 0.0207 by the end of training. \n",
    "\n",
    "This consistent downward trend indicates that the model is learning effectively and improving its reconstruction capabilities on the training data.\n",
    "\n",
    "Compared to the previous model trained with the higher learning_rate(*0.0001*), the validation loss (yellow line) exhibits several notable characteristics:\n",
    "\n",
    "- Reduced amplitude in fluctuations (ranging between 0.019-0.0205) compared to the previous model's wider range\n",
    "\n",
    "- More controlled spikes without the extreme peaks seen in the original model\n",
    "\n",
    "- Consistently lower values than the training loss, indicating good generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'model.pth'\n",
    "\n",
    "torch.save(model, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model (when using the model without training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pth')\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset):\n",
    "  predictions, losses = [], []\n",
    "  with torch.no_grad():\n",
    "    model = model.eval()\n",
    "    for batch_index, batch in enumerate(dataset):\n",
    "      x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "      output = model(x_batch)\n",
    "      loss = loss_function(output, y_batch)\n",
    "\n",
    "      predictions.append(output.cpu().numpy().flatten())\n",
    "      losses.append(loss.item())\n",
    "  return predictions, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, losses = predict(model, train_loader)\n",
    "sns.histplot(losses, bins=50, kde=True).set_title(\"Loss histogram of train data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a threshold by analyzing the loss histogram\n",
    "THRESHOLD = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Using the threshold, we can turn the problem into a simple *binary classification task*:\n",
    "\n",
    "- If the reconstruction loss is below the threshold, classify it as **normal data**\n",
    "\n",
    "- Alternatively, if the loss is higher than the threshold, classify it as an **anomaly**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal DC Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redictions, pred_losses = predict(model, test_loader)\n",
    "sns.histplot(pred_losses, bins=50, kde=True).set_title(\"Loss histogram of test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = sum(l <= THRESHOLD for l in losses)\n",
    "print(f'Correct normal predictions: {correct}/{len(train_loader)} ({correct*100/len(train_loader)} %)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the **abnormal** DC diff dataset from Hugging Face and load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to the Hugging face\n",
    "login()\n",
    "\n",
    "# Load the abnormal dataset\n",
    "ab_dataset = load_dataset(\"Jake5/wmx_abnormal_data\") \n",
    "\n",
    "hg_abdf = pd.DataFrame(ab_dataset['train'])\n",
    "\n",
    "print(f\"Total Values: {hg_abdf.shape[0]}\")\n",
    "print(hg_abdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_abdf = prepare_dataframe_for_lstm(hg_abdf, lookback)\n",
    "shifted_abdf_as_np = shifted_abdf.to_numpy()\n",
    "shifted_abdf_as_np = scaler.fit_transform(shifted_abdf_as_np)\n",
    "\n",
    "X_ab = shifted_abdf_as_np[:, 1:]\n",
    "y_ab = shifted_abdf_as_np[:, 0]\n",
    "\n",
    "X_ab = X_ab.reshape((-1, lookback, 1))\n",
    "y_ab = y_ab.reshape((-1, 1))\n",
    "\n",
    "X_ab = torch.tensor(X_ab).float()\n",
    "y_ab = torch.tensor(y_ab).float()\n",
    "\n",
    "abnormal_dataset = TimeSeriesDataset(X_ab, y_ab)\n",
    "abnormal_loader = DataLoader(abnormal_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, pred_losses = predict(model, abnormal_loader)\n",
    "sns.histplot(pred_losses, bins=50, kde=True).set_title(\"Loss histogram of abnormal data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = sum(l > THRESHOLD for l in pred_losses)\n",
    "print(f'Correct anomaly predictions: {correct}/{len(abnormal_dataset)} ({correct*100/len(abnormal_dataset)} %)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_predictions = np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_LEN = 4000\n",
    "time_steps = [i for i in range(X_LEN)]\n",
    "\n",
    "plt.plot(time_steps, y_ab[:X_LEN], label='Actual DcDiffAvg')\n",
    "plt.plot(time_steps, flattened_predictions[:X_LEN], label='Predicted DcDiffAvg')\n",
    "\n",
    "batch_ranges = []\n",
    "    \n",
    "for idx, loss in enumerate(pred_losses):\n",
    "    if idx * batch_size > X_LEN:\n",
    "        break\n",
    "    if loss > THRESHOLD:\n",
    "        batch_ranges.append((idx*batch_size, (idx+1)*batch_size))\n",
    "\n",
    "for start, end in batch_ranges:\n",
    "    plt.axvspan(start, end, color='red', alpha=0.2)\n",
    "\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('DcDiffAvg')\n",
    "plt.title('DcDiff anomalies (shown in Red)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining Considerations\n",
    "\n",
    "- Some validation volatility still exists, though significantly reduced\n",
    "\n",
    "- The training loss could potentially benefit from further fine-tuning to achieve even smoother convergence\n",
    "\n",
    "- The model might benefit from a longer training period to see if the trend continues to improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [Time Series Anomaly Detection Tutorial with PyTorch in Python | LSTM Autoencoder for ECG Data](https://www.youtube.com/watch?v=qN3n0TM4Jno)\n",
    "- [Amazon Stock Forecasting in PyTorch with LSTM Neural Network (Time Series Forecasting) | Tutorial 3](https://www.youtube.com/watch?v=q_HS4s1L8UI&t=5s)\n",
    "- [\\[NHN Cloud make IT 2023\\] 시계열 데이터 속에 숨어있는 이상 징후를 찾는 딥 러닝 기술](https://www.youtube.com/watch?v=bg2e60IZ40Q)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
